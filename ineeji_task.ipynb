{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PFrE4bJKx967"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y, seq_length):  # seq_length: n개월 간\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        self.y = torch.FloatTensor(y)\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.X[idx:idx+self.seq_length], self.y[idx+self.seq_length])"
      ],
      "metadata": {
        "id": "dnq82ITfx_qL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_model(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, n_layers=2, dropout=0.2, output_dim=1):\n",
        "        super().__init__()\n",
        "        self.batch_norm = nn.BatchNorm1d(input_dim)\n",
        "\n",
        "        # Bidirectional LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if n_layers > 1 else 0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # 시퀀스 가중치\n",
        "        self.seq_weight = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "        # FC layer\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        seq_len = x.size(1)\n",
        "\n",
        "        # batch normalization으로 입력 처리\n",
        "        x = x.view(-1, x.size(-1))\n",
        "        x = self.batch_norm(x)\n",
        "        x = x.view(batch_size, seq_len, -1)\n",
        "\n",
        "        # LSTM 실행\n",
        "        lstm_out, _ = self.lstm(x)  # 출력 크기: [batch_size, seq_len, hidden_dim * 2]; 양방향이기 때문에 출력이 두 개\n",
        "\n",
        "        # 시퀀스 가중치 계산\n",
        "        seq_weights = torch.softmax(self.seq_weight(lstm_out), dim=1)  # softmax: 가중치 정규화 목적\n",
        "        output = torch.sum(seq_weights * lstm_out, dim=1)  # 가중 합계로 시퀀스를 하나의 벡터로 압축\n",
        "\n",
        "        # FC layers\n",
        "        output = self.fc_layers(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "z5LXZex0yBC7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Absolute Percentage Error 계산\n",
        "\n",
        "def calculate_mape(y_true: np.ndarray, y_pred: np.ndarray) -> Dict:\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "\n",
        "    # 평균 예측값 사용 (3개월 예측 평균)\n",
        "    if y_pred.ndim > 1:\n",
        "        y_pred = np.mean(y_pred, axis=1)\n",
        "\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
      ],
      "metadata": {
        "id": "0AGixHJtyDry"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(train, test):\n",
        "    # 데이터 불러오기\n",
        "    train_data = pd.read_csv(train)\n",
        "    test_data = pd.read_csv(test)\n",
        "\n",
        "    # 데이터의 날짜 범위 확인\n",
        "    print(\"Training data date range:\", train_data['date'].iloc[0], \"to\", train_data['date'].iloc[-1])\n",
        "    print(\"Test data date range:\", test_data['date'].iloc[0], \"to\", test_data['date'].iloc[-1])\n",
        "\n",
        "    # 날짜 처리\n",
        "    train_data['date'] = pd.to_datetime(train_data['date'])\n",
        "    test_data['date'] = pd.to_datetime(test_data['date'])\n",
        "\n",
        "    # # 시계열 특성 추가 (feature engineering)\n",
        "    # 1. 계절성(월별) 특성\n",
        "    train_data['month'] = train_data['date'].dt.month\n",
        "    test_data['month'] = test_data['date'].dt.month\n",
        "\n",
        "    # 2. 이동평균 특성\n",
        "    for window in [3, 6, 9, 12]:  # 3, 6, 12개월 동안의 가격 이동 평균 (moving average)\n",
        "        train_data[f'price_ma{window}'] = train_data['Natural_Gas_US_Henry_Hub_Gas'].rolling(window=window).mean()\n",
        "        test_data[f'price_ma{window}'] = test_data['Natural_Gas_US_Henry_Hub_Gas'].rolling(window=window).mean()\n",
        "\n",
        "    # 3. 가격 변화율\n",
        "    train_data['price_change'] = train_data['Natural_Gas_US_Henry_Hub_Gas'].pct_change()  # (현재 가격 - 이전 가격) / 이전 가격\n",
        "    test_data['price_change'] = test_data['Natural_Gas_US_Henry_Hub_Gas'].pct_change()\n",
        "\n",
        "    # 4. 변동성 (표준편차)\n",
        "    for window in [3, 6, 9, 12]:\n",
        "        train_data[f'price_volatility_{window}'] = train_data['Natural_Gas_US_Henry_Hub_Gas'].rolling(window=window).std()\n",
        "        test_data[f'price_volatility_{window}'] = test_data['Natural_Gas_US_Henry_Hub_Gas'].rolling(window=window).std()\n",
        "\n",
        "    # 새로 만든 특성만 저장한 파일 생성\n",
        "    new_features = ['date', 'month', 'price_ma3', 'price_ma6', 'price_ma9', 'price_ma12', 'price_change', 'price_volatility_3', 'price_volatility_6', 'price_volatility_9', 'price_volatility_12']\n",
        "\n",
        "    new_train_data = train_data[new_features].copy()\n",
        "    new_train_data.to_csv('new_data_train.csv', index=False)\n",
        "\n",
        "    new_test_data = test_data[new_features].copy()\n",
        "    new_test_data.to_csv('new_data_test.csv', index=False)\n",
        "\n",
        "\n",
        "    # 특징\n",
        "    feature_cols = [\n",
        "        # 에너지 가격 변수\n",
        "        'Brent_Crude_Oil', 'WTI_Crude_Oil', 'Electricity_Price_USA',\n",
        "\n",
        "        # 공급과 수요\n",
        "        'Natural_Gas_Imports_From_Canada_USA', 'Natural_Gas_Imports_USA',\n",
        "        'Natural_Gas_Total_Consumption_USA', 'Natural_Gas_Rotary_Rig_Count_USA',\n",
        "        'Total_Natural_Gas_Marketed_Production_USA', 'Total_Natural_Gas_Underground_Storage_Volume_USA',\n",
        "\n",
        "        # 소비자 물가 지수\n",
        "        'CPI_Energy_Seasonally_Adjusted_USA', 'CPI_Index_Seasonally_Adjusted_USA',\n",
        "\n",
        "        # 블룸버그 천연 가스 , 미국 달러 지수\n",
        "        'BCOMNG_INDX', 'DXY_INDX',\n",
        "\n",
        "        # feature engineering\n",
        "        'price_ma3', 'price_ma6', 'price_ma9', 'price_ma12', 'price_change', 'price_volatility_3', 'price_volatility_6', 'price_volatility_9', 'price_volatility_12',\n",
        "\n",
        "        # 추가 변수\n",
        "        'FEDFUNDS', 'INDPRO', 'ONI',  # 'GPR', 'GPRHC_RUS', 'GPRHC_UKR', 'GPRHC_USA', 'GPRH', 'VIXCLS', 'GDP'\n",
        "    ]\n",
        "\n",
        "    # 측정할 값\n",
        "    target_col = 'Natural_Gas_US_Henry_Hub_Gas'\n",
        "\n",
        "    # 결측치 처리\n",
        "    for col in feature_cols + [target_col]:\n",
        "        train_data[col] = train_data[col].fillna(method='ffill').fillna(method='bfill')\n",
        "        test_data[col] = test_data[col].fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "    # 특징과 타겟 분리\n",
        "    X_train = train_data[feature_cols].values\n",
        "    y_train = train_data[target_col].values\n",
        "    X_test = test_data[feature_cols].values\n",
        "    y_test = test_data[target_col].values\n",
        "\n",
        "    # 스케일링\n",
        "    scaler_X = RobustScaler()\n",
        "    scaler_y = RobustScaler()\n",
        "\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
        "    X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "    return X_train_scaled, y_train_scaled, X_test_scaled, scaler_y, test_data['date'].values, y_test"
      ],
      "metadata": {
        "id": "77yb9g6RyGjP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "\n",
        "def train_model(X_train, y_train, n_epoch, seq_length) -> Tuple[LSTM_model, Dict]:\n",
        "    # 데이터셋 생성\n",
        "\n",
        "    train_dataset = SimpleTimeSeriesDataset(X_train, y_train, seq_length=seq_length)\n",
        "    train_size = int(len(train_dataset) * 0.8)\n",
        "    train_dataloader = DataLoader(torch.utils.data.Subset(train_dataset, range(train_size)), batch_size=32, shuffle=True)\n",
        "\n",
        "\n",
        "    X_valid = X_train[train_size:]\n",
        "    y_valid = y_train[train_size:]\n",
        "\n",
        "    valid_dataloader = None\n",
        "    if len(X_valid) > 6:  # check for valid length of the validation set\n",
        "      valid_dataset = SimpleTimeSeriesDataset(X_valid, y_valid, seq_length=seq_length)\n",
        "      valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "    # 모델 초기화\n",
        "    model = LSTM_model(input_dim=X_train.shape[1], hidden_dim=256, n_layers=4, dropout=0.3)\n",
        "\n",
        "    # 손실 함수와 옵티마이저\n",
        "    criterion = nn.HuberLoss(delta=1.0)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "    # 학습 기록\n",
        "    history = {'train_loss': [], 'valid_loss': []}\n",
        "\n",
        "    best_valid_loss = float('inf')\n",
        "    patience = 10\n",
        "    patience_counter = 0\n",
        "\n",
        "    # 학습\n",
        "    model.train()\n",
        "    for epoch in range(n_epoch):\n",
        "        total_loss = 0\n",
        "        for X_batch, y_batch in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(X_batch)\n",
        "\n",
        "            loss = criterion(y_pred, y_batch.view(-1, 1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_dataloader)\n",
        "        history['train_loss'].append(avg_loss)\n",
        "\n",
        "        # validation\n",
        "        if valid_dataloader: # check if validation loader is defined\n",
        "            model.eval()\n",
        "            total_valid_loss = 0\n",
        "            with torch.no_grad():\n",
        "              for X_batch, y_batch in valid_dataloader:\n",
        "                y_pred = model(X_batch)\n",
        "                valid_loss = criterion(y_pred, y_batch.view(-1, 1))\n",
        "                total_valid_loss += valid_loss.item()\n",
        "\n",
        "            avg_valid_loss = total_valid_loss / len(valid_dataloader)\n",
        "            history['valid_loss'].append(avg_valid_loss)\n",
        "\n",
        "            scheduler.step(avg_valid_loss)\n",
        "\n",
        "            # Early stoptting\n",
        "            if avg_valid_loss < best_valid_loss:\n",
        "              best_valid_loss = avg_valid_loss\n",
        "              patience_counter = 0\n",
        "            else:\n",
        "              patience_counter += 1\n",
        "\n",
        "            if patience_counter >= patience:\n",
        "              print(f'Early stopping at epoch {epoch+1}')\n",
        "              break\n",
        "\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{n_epoch}], Training Loss: {avg_loss:.4f}, Valid Loss: {avg_valid_loss:.4f}')\n",
        "\n",
        "    # Loss 그래프\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['train_loss'], label='Train Loss')\n",
        "\n",
        "    if 'valid_loss' in history:  # if validation loss was recorded\n",
        "        plt.plot(history['valid_loss'], label='Validation Loss')\n",
        "\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('training_history.png')\n",
        "    plt.close()\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "IFWldkWuyIJA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 앙상블\n",
        "\n",
        "def train_ensemble_models(X_train, y_train, n_epochs, sequence_lengths=[3, 6, 12]):\n",
        "    models = {}\n",
        "    histories = {}\n",
        "\n",
        "    # 각 시퀀스 길이별로 모델 학습\n",
        "    for seq_len in sequence_lengths:\n",
        "        print(f\"\\nTraining model with sequence length {seq_len}\")\n",
        "        model, history = train_model(X_train, y_train, n_epochs, seq_len)  # seq_length 파라미터 추가 필요\n",
        "        models[seq_len] = model\n",
        "        histories[seq_len] = history\n",
        "\n",
        "    return models, histories\n",
        "\n",
        "def ensemble_predictions(models, X_test, scaler_y, weights=None):\n",
        "    all_predictions = []\n",
        "\n",
        "    # 각 시점에 대해\n",
        "    for i in range(len(X_test)):\n",
        "        model_predictions = []\n",
        "\n",
        "        # 각 모델의 예측값 얻기\n",
        "        for seq_len, model in models.items():\n",
        "            # 시퀀스 길이에 따른 패딩 처리\n",
        "            if i < seq_len - 1:\n",
        "                padding = np.tile(X_test[0], (seq_len - 1 - i, 1))\n",
        "                current_seq = np.vstack([padding, X_test[:i + 1]])\n",
        "            else:\n",
        "                current_seq = X_test[i - seq_len + 1:i + 1]\n",
        "\n",
        "            input_seq = torch.FloatTensor(current_seq).unsqueeze(0)\n",
        "            predictions = []\n",
        "\n",
        "            # 테스트 데이터의 월별 3개월 예측\n",
        "            for _ in range(3):\n",
        "                pred = model(input_seq)\n",
        "                scaled_pred = scaler_y.inverse_transform(pred.detach().numpy())[0]\n",
        "                predictions.append(scaled_pred[0])\n",
        "\n",
        "                current_seq = np.vstack([current_seq[1:], current_seq[-1]])\n",
        "                input_seq = torch.FloatTensor(current_seq).unsqueeze(0)\n",
        "\n",
        "            model_predictions.append(predictions)\n",
        "\n",
        "        # 앙상블 (가중 평균)\n",
        "        if weights is None:\n",
        "            weights = [1/len(models)] * len(models)  # 동일 가중치\n",
        "\n",
        "        ensemble_pred = np.average(model_predictions, axis=0, weights=weights)\n",
        "        all_predictions.append(ensemble_pred)\n",
        "\n",
        "    return all_predictions"
      ],
      "metadata": {
        "id": "8_i63qhoyK-m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 결과 분석\n",
        "\n",
        "def analyze_predictions(y_true: np.ndarray, y_pred: np.ndarray, dates: np.ndarray) -> Dict:\n",
        "    # 길이 확인 및 조정\n",
        "    min_len = min(len(y_true), len(y_pred), len(dates))\n",
        "\n",
        "    y_true = y_true[:min_len]\n",
        "    y_pred = y_pred[:min_len]\n",
        "    dates = dates[:min_len]\n",
        "\n",
        "    # 평균 예측값 사용 (3개월 예측 평균)\n",
        "    if y_pred.ndim > 1:\n",
        "        y_pred = np.mean(y_pred, axis=1)\n",
        "\n",
        "    # 메트릭 계산\n",
        "    metrics = {'mape': calculate_mape(y_true, y_pred)} # return mape as a dictionary\n",
        "\n",
        "    # 오차 분포 분석\n",
        "    errors = y_pred - y_true\n",
        "    error_stats = {\n",
        "        'mean_error': np.mean(errors),\n",
        "        'std_error': np.std(errors),\n",
        "        'max_error': np.max(np.abs(errors)),\n",
        "        'error_distribution': errors\n",
        "    }\n",
        "\n",
        "    # 시각화\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # 1. 실제 vs 예측 가격\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(dates, y_true, label='Actual')\n",
        "    plt.plot(dates, y_pred, label='Predicted')\n",
        "    plt.title('Actual vs Predicted Prices')\n",
        "    plt.legend()\n",
        "\n",
        "    # 2. 오차 분포\n",
        "    plt.subplot(2, 2, 2)\n",
        "    sns.histplot(errors, bins=30)\n",
        "    plt.title('Error Distribution')\n",
        "\n",
        "    # 3. 예측 vs 실제 가격 산점도\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.scatter(y_true, y_pred)\n",
        "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
        "    plt.title('Predicted vs Actual')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('prediction_analysis.png')\n",
        "    plt.close()\n",
        "\n",
        "    return {'metrics': metrics, 'error_stats': error_stats}"
      ],
      "metadata": {
        "id": "HHC0KGdfyMww"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # 데이터 준비\n",
        "    X_train, y_train, X_test, scaler_y, test_dates, y_test = prepare_data('data_train_new.csv', 'data_test_new.csv')\n",
        "\n",
        "    # 앙상블 모델 학습\n",
        "    sequence_lengths = [3, 6, 9, 12]  # 3, 6, 9, 12개월;\n",
        "    models, histories = train_ensemble_models(X_train, y_train, 100, sequence_lengths)\n",
        "\n",
        "    # 앙상블 예측\n",
        "    weights = [0.1, 0.2, 0.3, 0.4]  # 옵션: [0.1, 0.2, 0.3, 0.4], [0.15, 0.2, 0.3, 0.35]; [0.1, 0.15, 0.35, 0.4]\n",
        "    all_predictions = ensemble_predictions(models, X_test, scaler_y, weights)\n",
        "\n",
        "    # 예측 분석\n",
        "    test_predictions = np.array(all_predictions)[:-2]  # 마지막 두 개 제외 (3개월 예측 때문)\n",
        "    y_test_subset = y_test[:len(test_predictions)]\n",
        "    dates_subset = test_dates[:len(test_predictions)]\n",
        "    analysis_results = analyze_predictions(y_test_subset, test_predictions, dates_subset)\n",
        "\n",
        "    print(f\"MAPE: {analysis_results['metrics']['mape']:.2f}%\")\n",
        "\n",
        "    # 제출 파일 생성\n",
        "    dates = pd.to_datetime(test_dates)\n",
        "    submission_data = []\n",
        "\n",
        "    # 테스트 데이터의 각 날짜에 대해 예측값 저장\n",
        "    for date, preds in zip(dates, all_predictions):\n",
        "        if preds[0] is not None:  # 예측이 가능한 경우만 포함\n",
        "            submission_data.append({\n",
        "                'date': date.strftime('%Y-%m-%d'),\n",
        "                'pred_1': preds[0],\n",
        "                'pred_2': preds[1],\n",
        "                'pred_3': preds[2]\n",
        "            })\n",
        "\n",
        "    submission = pd.DataFrame(submission_data)\n",
        "    submission.to_csv('submission.csv', index=False)\n",
        "    print(\"\\nSubmission file shape:\", submission.shape)\n",
        "    print(\"First date in submission:\", submission['date'].iloc[0])\n",
        "    print(\"Last date in submission:\", submission['date'].iloc[-1])\n",
        "    print(\"Predictions saved to submission.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLa-6rgNyOba",
        "outputId": "5b78250d-501a-4001-a309-bb14fe89a00f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data date range: 2003-09-30 to 2020-08-31\n",
            "Test data date range: 2020-09-30 to 2024-08-31\n",
            "\n",
            "Training model with sequence length 3\n",
            "Epoch [50/100], Training Loss: 0.0125, Valid Loss: 0.0033\n",
            "Epoch [100/100], Training Loss: 0.0117, Valid Loss: 0.0032\n",
            "\n",
            "Training model with sequence length 6\n",
            "Epoch [50/100], Training Loss: 0.0105, Valid Loss: 0.0031\n",
            "Epoch [100/100], Training Loss: 0.0103, Valid Loss: 0.0030\n",
            "\n",
            "Training model with sequence length 9\n",
            "Epoch [50/100], Training Loss: 0.0089, Valid Loss: 0.0031\n",
            "Epoch [100/100], Training Loss: 0.0080, Valid Loss: 0.0031\n",
            "\n",
            "Training model with sequence length 12\n",
            "Epoch [50/100], Training Loss: 0.0086, Valid Loss: 0.0039\n",
            "Epoch [100/100], Training Loss: 0.0084, Valid Loss: 0.0039\n",
            "MAPE: 22.52%\n",
            "\n",
            "Submission file shape: (48, 4)\n",
            "First date in submission: 2020-09-30\n",
            "Last date in submission: 2024-08-31\n",
            "Predictions saved to submission.csv\n"
          ]
        }
      ]
    }
  ]
}
